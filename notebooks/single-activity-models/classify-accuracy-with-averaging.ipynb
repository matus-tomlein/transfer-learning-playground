{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "width = 12\n",
    "height = 7\n",
    "plt.rcParams[\"figure.figsize\"] = (width, height)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tflscripts\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "configuration = tflscripts.read_configuration()\n",
    "df = pd.read_pickle('results_filtered.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "without_transfer = df.loc[df.type_of_transfer == 'No transfer']\n",
    "\n",
    "def apply_accuracy_without_transfer(x):\n",
    "    queried = without_transfer.query('source_device_name == \"{}\" & source_dataset == \"{}\" & features == \"{}\" & label == \"{}\" & classifier == \"{}\"'.format(\n",
    "        x['source_device_name'],\n",
    "        x['source_dataset'],\n",
    "        x['features'],\n",
    "        x['label'],\n",
    "        x['classifier']\n",
    "    ))\n",
    "\n",
    "    if len(queried) > 0:\n",
    "        return queried['accuracy_positive'].iloc[0]\n",
    "\n",
    "    return -1\n",
    "\n",
    "df['accuracy_without_transfer'] = df.apply(apply_accuracy_without_transfer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['accuracy_without_transfer_r'] = df['accuracy_without_transfer'].round(1)\n",
    "df['accuracy_negative_r'] = df['accuracy_negative'].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_accuracy_bin(acc):\n",
    "    bins = [\n",
    "        [0.0, 0.75],\n",
    "        [0.75, 1.0]\n",
    "    ]\n",
    "    return [i for i, b in enumerate(bins) if b[0] <= acc and b[1] >= acc][0]\n",
    "\n",
    "df['accuracy_bin'] = [to_accuracy_bin(a) for a in df['accuracy_positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = df.loc[df.type_of_transfer != 'No transfer']\n",
    "\n",
    "test_split = 0.33\n",
    "msk = np.random.rand(len(filtered)) <= test_split\n",
    "train = filtered[msk]\n",
    "test = filtered[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'features',\n",
    "    'classifier',\n",
    "    'label',\n",
    "    'samples',\n",
    "    'type_of_transfer',\n",
    "    'accuracy_without_transfer_r',\n",
    "    'accuracy_negative_r'\n",
    "]\n",
    "\n",
    "def apply_avg(x):\n",
    "    queried = df.query('features == \"{}\" & classifier == \"{}\" & label == \"{}\" & samples == {} & type_of_transfer == \"{}\" & accuracy_without_transfer_r == \"{}\" & accuracy_negative_r == \"{}\"'.format(\n",
    "        x['features'],\n",
    "        x['classifier'],\n",
    "        x['label'],\n",
    "        x['samples'],\n",
    "        x['type_of_transfer'],\n",
    "        x['accuracy_without_transfer_r'],\n",
    "        x['accuracy_negative_r']\n",
    "    ))\n",
    "\n",
    "    if len(queried) > 0:\n",
    "        median = queried['accuracy_positive'].median()\n",
    "        return median\n",
    "\n",
    "    return -1\n",
    "\n",
    "aggregated = train[columns]\n",
    "aggregated = aggregated.drop_duplicates()\n",
    "aggregated['accuracy_positive'] = aggregated.apply(apply_avg, axis=1)\n",
    "aggregated['accuracy_bin'] = [to_accuracy_bin(a) for a in aggregated['accuracy_positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937396469983\n"
     ]
    }
   ],
   "source": [
    "def to_x_and_y(filtered):\n",
    "    X = filtered[columns]\n",
    "    y = filtered['accuracy_bin']\n",
    "\n",
    "    X = [dict(r.iteritems()) for _, r in X.iterrows()]\n",
    "    y = y.values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = to_x_and_y(aggregated)\n",
    "X_test, y_test = to_x_and_y(test)\n",
    "\n",
    "ppl = Pipeline([\n",
    "    ('vect', DictVectorizer()),\n",
    "    ('impute', Imputer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "ppl.fit(X_train, y_train)\n",
    "predicted = ppl.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15208,   262],\n",
       "       [  834,  1203]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.Series(predicted)\n",
    "predictions.index = test.index\n",
    "df['predictions'] = predictions\n",
    "\n",
    "df.to_pickle('results_with_accuracy_classified.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97     15470\n",
      "          1       0.82      0.59      0.69      2037\n",
      "\n",
      "avg / total       0.93      0.94      0.93     17507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
